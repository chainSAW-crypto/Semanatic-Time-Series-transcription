{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c5c5ac",
   "metadata": {},
   "source": [
    "Semantic Chunking of a YouTube Video \n",
    "youtube video link - https://youtu.be/Sby1uJ_NFIY?si=QCGzzQptwQgW78o9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b74d2e6",
   "metadata": {},
   "source": [
    "**Choice of Open source model**\n",
    "\n",
    "To achieve the highest precision and ease of integration with various applications, we have chosen **OpenAI's Whisper speech recognition model**. The objectives of this task are: 1. Precise time-aligned transcription of audio, and 2. Semantic chunking with each chunk being less than 15 seconds in length. Whisper excels in its ability to provide a complete transcription and then segment the text into precise time-series data.\n",
    "\n",
    "In contrast, other available models, such as Mozilla DeepSpeech and Kaldi, require audio files to be sequentially split into smaller, equal time segments and then processed individually to achieve similar accuracy in time-series transcription. This approach is time-consuming and data-intensive, especially for large audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe9bec8",
   "metadata": {},
   "source": [
    "Setting up pre- requisites for whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb66331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in c:\\users\\tanay\\anaconda3\\lib\\site-packages (20231117)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from openai-whisper) (10.1.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from openai-whisper) (0.7.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from openai-whisper) (1.23.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from openai-whisper) (4.64.1)\n",
      "Requirement already satisfied: torch in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from openai-whisper) (2.1.0)\n",
      "Requirement already satisfied: numba in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from openai-whisper) (0.56.4)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from numba->openai-whisper) (0.39.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from numba->openai-whisper) (65.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from tiktoken->openai-whisper) (2.32.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from tiktoken->openai-whisper) (2022.7.9)\n",
      "Requirement already satisfied: networkx in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (3.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (3.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (1.11.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from tqdm->openai-whisper) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from jinja2->torch->openai-whisper) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from sympy->torch->openai-whisper) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install -U openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa93df60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\tanay\\appdata\\local\\temp\\pip-req-build-c5diuqie\n",
      "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from openai-whisper==20231117) (4.64.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from openai-whisper==20231117) (0.7.0)\n",
      "Requirement already satisfied: torch in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from openai-whisper==20231117) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from openai-whisper==20231117) (1.23.5)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from openai-whisper==20231117) (10.1.0)\n",
      "Requirement already satisfied: numba in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from openai-whisper==20231117) (0.56.4)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from numba->openai-whisper==20231117) (0.39.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from numba->openai-whisper==20231117) (65.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from tiktoken->openai-whisper==20231117) (2.32.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from tiktoken->openai-whisper==20231117) (2022.7.9)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from torch->openai-whisper==20231117) (2024.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from torch->openai-whisper==20231117) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from torch->openai-whisper==20231117) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from torch->openai-whisper==20231117) (2.8.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from torch->openai-whisper==20231117) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from torch->openai-whisper==20231117) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from tqdm->openai-whisper==20231117) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from jinja2->torch->openai-whisper==20231117) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from sympy->torch->openai-whisper==20231117) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\TANAY\\AppData\\Local\\Temp\\pip-req-build-c5diuqie'\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/openai/whisper.git "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff25d22",
   "metadata": {},
   "source": [
    "According to the prerequisites to run OpenAI whisper in the system is installing and seting up rust in the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1248765f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools-rust in c:\\users\\tanay\\anaconda3\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: setuptools>=62.4 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from setuptools-rust) (65.6.3)\n",
      "Requirement already satisfied: tomli>=1.2.1 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from setuptools-rust) (2.0.1)\n",
      "Requirement already satisfied: semantic-version<3,>=2.8.2 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from setuptools-rust) (2.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install setuptools-rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec52457c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools-rust in c:\\users\\tanay\\anaconda3\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: tomli>=1.2.1 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from setuptools-rust) (2.0.1)\n",
      "Requirement already satisfied: setuptools>=62.4 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from setuptools-rust) (65.6.3)\n",
      "Requirement already satisfied: semantic-version<3,>=2.8.2 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from setuptools-rust) (2.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install setuptools-rust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd2a6d",
   "metadata": {},
   "source": [
    "**Making predictions**\n",
    "\n",
    "There are five model sizes of Whisper. For this task, the base model is used, which takes in account 74 There are five model sizes of Whisper. For this task, the base model is used, which utilizes 74 million parameters and requires 1 GB of VRAM. In comparison, the large model utilizes 1.55 billion parameters and requires 10 GB of VRAM, making the base model 16 times faster. It is ideal for videos where the voice and pronunciations are clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "925bcfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31df786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TANAY\\anaconda3\\lib\\site-packages\\whisper\\transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Congratulations to you Mr. Raghavan for that. Thank you so much for joining us. Over to you. Hi everybody. How are you? Okay I am not hearing this at all. It's like a post lunch energy downer or something. Let's hear it. Are you guys awake? All right you better be because we have a superstar guest here. You heard the 41 million dollars and I didn't hear honestly anything she said after that. So we're going to ask for about 40 million dollars from him by the end of this conversation. But let's get started. I want to introduce Vivek and Pratius, she's co-founder who's not here. We wanted to start with a playing a video of what OpenHathe does. I encourage all of you to go to the website, www.severalm.ai and check it out. But let me start by introducing Vivek. Vivek is a dear friend and he is very, very modest. One of the most modest guys that I know. But his personal journey Vivek, you've got a PhD from Carnegie Mellon. You've sat in and sold the company to Magma. Vivek and I moved back to India from both in the valley on the same day actually. And you've been in India for the last 16 years. And what most people don't know is your journey at Adhar. He spent 13 years selflessly at Adhar. Nobody would have heard of him. But he was a pioneering technology visionary behind Adhar which we all take for granted today. So please give it out. Honestly when people, when I think of selfless service, truly selfless service, I always think of Vivek. And since then he also was at AI for Bharat which we're going to touch on where he met Pratiush's other co-founder. Pratiush had a PhD from ETH at Zurich. He was in the IBM research. He was at Microsoft Research playing a key role and a faculty at IIT Madras and at AI for Bharat. So that's a little brief introduction about them. These guys are modest, modest engineers. So they don't do their own hon. So forgive me for tooting their hon in this case. But let's jump right in about the money. Funding. 41 million bucks man. That's a lot of money. Every entrepreneur here is saying what the hell did these guys do? What did the investors see to write such a big check? No, I think it's a trend of what's going on in India. I think that for the very first time, I think the investors have looked at, let's try and build something deep-tech out of the country. And let's try to figure out how to build something as a foundational technology out of the country. And that's really what's really exciting. And I think that about as Balav was mentioning for the past 15 years, I've been working in both digital public infrastructure and kind of non-profit kind of things. But when this whole thing of generative AI came about, we said, okay, how can I actually make a difference in this space? And I said, maybe this is the opportunity to actually come out and really build something. And the only way that we realize that you can do it is actually in the private sector. And I think that's, and then when we went out there and we said, we want to build something, which is a continuation. And fundamentally, the question is the reason of what we want to do at Server May I is we want to basically make generative AI available and accessible to the people in the country. And that's the intent. And when we said that we want to do this, there was a resonance in the investment community. And I think it's a responsibility to really to show that something like this can be built out of India. So we see that as as as as confidence and a responsibility. And I also hope it's a trend that that you know that there are many more people like like us who are backed. Because if you look at it, maybe it's a large number in a you know in the Indian context, but in the global context, I think there is just there should be many, many more entrepreneurs who are back to do things in India. I'm going to come back to the many more entrepreneurs. I'm obviously going to ask you about Bhavish's Krutram. So we're going to come back to that question. But again, 41 million dollars. I mean, all of what you said, you know, two million dollars, you know, that's a good no-one of money for a startup which you know, which has not yet built anything. What are you going to do with all this money? I can call the problem. I can have a perfect solution for the problem. I think in the last week I've got lots of calls, lots of people telling me how I can do. No, but I know you first. I'll be landed in the country the same day. I'm in the front of the queue. No, but but but honestly, I think the key thing in this is is to putting together an amazing team. And we actually have an amazing team, but we believe that it is talent that will drive this kind of thing. And so it is it is to get get key talent. And of course, the other thing is compute. This is extremely expensive compute wise to actually do these kinds of things. And I think that those are the two primary things that that you know, we'd use this for. Okay. I'm computing in my own head as an entrepreneur talent. Okay, you have like 2015 people. How much are you paying these guys? But okay, well, you want that. But let's talk about what you guys actually built. What is what is open Hathi? Well, how would you explain open Hathi to many people here who might not have no, no, about it? So I think open Hathi is so, so first of all, right? We come from I personally come from the open source ecosystem and we, we, and also from the DPI ecosystem. So we believe that for this to work, we need the ecosystem to be successful. And as a result of that, one of the first things we did was, hey, there are these open source large language models that exist, right? I mean, everybody knows about the Lama family from Mehta. They also know there are others like Mr. Al, there are a bunch, bunch of open source, you know, large language models. And then we said, is there any way that take an existing open source model and teach it language skills, right? I mean, you know, language. And that is really the, you know, what we decide, what we said that can we do something like that? And is this a, you know, relatively frugal way of actually, you know, making models, you know, work in diverse languages because the truth is still today. I mean, if you look at the amount of data and knowledge, it is still English dominates these things. And I think that how do you actually take and make it understand Indian language, understand Indian context and all of those things in actually a, in an efficient way. And therefore, this was an attempt through that. And it's a open Hathi is, you know, is currently based on Ralama 7 billion model, but we'll be releasing many more models in different languages, different sizes and things like that as part of this, as part of this series. And of course, you know, we will be building further models on those and doing other things to actually, and we'll also have end points that people can use. So therefore, it's not, it's definitely, you know, something that people can can can use to things. And the, that's, that's the essence of what, what this open Hathi is. So what does it mean to people in the audience here who are either doing their own startups or a business or, or, or developers, how should they look at OpenAI? Oh, sorry, Sarvam, not OpenAI. No, yeah, no, no, I think, I think the way you look at it is that we, we are one of the important things that we are doing is we're not just building models. We are also going to be building a platform, a platform for developers where you can actually use a combination of various different kinds of models, some which are from us, some which are open source, some which we not be open source, and actually to actually pull together and figure out how to deploy, you know, generative AI applications at scale and understand and evaluate their performance in an efficient manner. And that's something that we are planning to do at this, and, and this platform is, you know, in the next couple of months, we'll be coming out there, it will be available to developers, but of course those who want to start with the open source things and hack with that, of course, please go ahead and do that as well. That's, that's phenomenal. But how does it compare to OpenAI itself or Google? See, at least the things that we are doing now, right? I mean, one of the things that when we thought about building Sarvam, we said we want to build a full stack generative AI company, and different people have, and our understanding of a full stack is that we need to know how to train models from scratch. We need to know how to kind of figure out how to deploy models to solve real world use cases, and we need to play in the ecosystem to make sure that we can actually deploy population scale applications, right? So we were thinking about all of these things, but still the models we were talking about are, you know, fairly small models, they are fairly small models, right? The seven to maybe up to 70 billion kind of range we're talking about. While these models like OpenAI and Google are obviously much bigger models, right? But we want to, but you know, we want to understand the techniques and be able to build that muscle to do all of these things to make it to make it available to people. Now those models are, I mean, as I said, you know, I think that there is space for all of those things, and I think as even Sridhar was talking about earlier in the day, we believe that these smaller models can do very, I mean, many, many kind of domain specific tasks extremely well, probably even better than the larger models, and that is really one of the key areas. And so the further value of these kinds of things, right? We are not aiming in these most set of models to build any AGI, right? That's not our goal here. Our goal is to make things that work extremely well for domain specific use cases or increase accessibility through language and all of those kinds of things. And obviously all of this unique to India. But what is unique about India? I mean, like, what is, is anything special in our ecosystem that makes a small model focused with Indian languages better for more suited for our problems? So I think that, I mean, there are quite a few things that are unique about India, right? The first thing is, I think, that we are a voice first nation. So therefore, I think voice has to be the core to doing things. The other thing, of course, India is extremely, it's a cost conscious country from from from from a cost perspective. Now there, I would say that there are lots of interesting use cases where you can use OpenAI and the cost structure works that when when we're depending on your application. But when you want to scale things to a massive level and make it work, then then you have to figure out how small models work. So that's something that is also specific to India. The third thing which is specific to India is really the success that India has had in building all this digital public infrastructure. When you add the AI layer on top of it, then you can actually get dramatic, you know, dramatic, I think, multiplicative combinatorial effects based on doing things like that. That's a phenomenal point. Like, you know, it's like dpi to the power of AI almost in some ways. And as a part of other building other, no better person than you. So in summary, what I'm hearing is small models specialized with trained with Indic specific language data suited for Indian problems at a compelling cost point will be suited for us. We're not solving some world autonomous vehicles or some complex problem. We're solving some basic problems specifically focused with on voice with multiple languages. That is what you see as the future. Am I paraphrasing this correctly? No, yeah. So I think that certainly, I mean, voice and Indian languages are an important part of our strategy, but we will be building custom models to solve various other kinds of problems as well. That's not just limited to, I think, in different domains, working in different domains, making building things based on unique data that enterprises have and things like that. So that's something that we'll also look at. Fair enough. So coming back to the elephant in the room, no fun intended with OpenHathi, what about Bavishakarwal and Krutrim? What does your take on that? I think it's great. I think it's absolutely wonderful, right? I mean, the fact that the technology AI is so important that we need multiple people working on it. The fact that there are other people thinking is actually validates that this is an important problem to be solved. And I think that we need everybody to come together and do that. So I really welcome that. I think it's great. And I think that there'll be different people who will have different takes as to how to solve this kind of problem. And hopefully, as a result of that, the entire ecosystem benefits. One more question, and then I want to talk about some of the predictions that you've boldly made. So Vivek, I usually ask people about what do you think the future will be and everybody usually hedges? I ask Vivek, what do you think is going to happen by December 2024? What do you think sitting in this room one year later, we can expect? And you made three bold predictions. So I want to talk about that before that I have one last question. What are the top three applications that you think are relevant for India? You would see the talk about medical. What do you think the top three apps are for India for AI? So I think that as you said, things like education and medical are clearly areas where I think that things can be leveraged. The whole idea of all these kind of the DPI aspect of it is another major application where things can happen. And here I'm talking about country-specific. And I think the whole idea which we've also talked about was the concept of software. And I think that and clearly we have a very large software industry and how to reimagine those things in this context is also something that's going to be big. Fair enough. Are you guys ready for Vivek Ragavan's bold predictions? Yes? No, I'm not hearing any. This is like a big deal. He's one of the smartest guys that I know. He wants to make three predictions. You don't want to hear it. All right. So I asked him, what do you think, you know, year later, what do you think we can expect? And he came up with three things and usually people give very blind answers when you ask questions like this because they don't want to be caught wrong. Not Vivek. Vivek is bold. So he basically said three things and I'm going to list out the three things and then he's asking about it. So number one, he says, I will prefer to talk to an automated customer service than a real person because they'll give me a better answer. So that is Vivek Ragavan's prediction. Number one. So number two is that when everybody is talking about a GPU shortage, Vivek predicts that there'll be a GPU glutton India. He thinks there'll be too much GPU. So if you want a short and media stock, this is a good time. And number three, which was extremely unexpected, he said some companies will suddenly die. Okay. So Vivek, these are not what I expected. So you want to quickly talk about each of them. Why you just came up with these and then we'll throw the audience questions. So I don't think I quite said it the way that that Balai's marketing ideas. But it's interesting. But I think the first thing that we said is I think that and I don't think that this is I think there will come a time when you know in areas of customer service, et cetera. When you want to do something very specific, today when you call when you call some kind of a bot, you actually end up, you mostly try to disconnect the call or you're extremely upset that you're talking to a bot. But I think that there will come a time and I'm predicting it sooner than later that you will actually get better responses from the bot and what the human representative that at least the average human representative that you could talk to could give. And I think that that's just a I just said that that there will come a time where you know it's not a human you're talking to. But it's probably more likely to solve your intent than the human person. That's just something that I think could happen. Definitely controversial. But we'll let it go. What about the GPU glut? No, I don't think that so I think that the fact that there is a tremendous shortage right now, I think that shortage will ease because that is how the cycles of things go, right? When I think the fact that there was such a severe shortage last year, you know basically caused a number of different players to ramp up in various kinds of forms. And I think that that will always go in a cycle. But you may we may find out that there are many many more interesting problems that people will be able to solve. I still remember, you know we were at at a at a at a genie event in Bangalore and we were talking to people and we said you know how many people have access to you know four a hundred. This was the question that I asked and nobody in the room and these are all extremely enthusiastic genie people and nobody had access. And I think that thing is going to change. You will be able to get these kinds of things and people who want to hack and do things will have access to these things at in without you know having to write a you know a major check check. So the way is also a semi-conductor guy before he went into the other. So I would take his predictions very seriously. So I don't know what I I'm going to sell my immediate stock. I would not do that. But that's not what I said. I want to blame you for this. But the third one is pretty strange. You know companies are born companies die but you said some companies will suddenly die. What does that mean? No I think see I think the the interesting thing is and I think that if it comes back to the fundamental nature of AI. AI is a tool right and you have to use that and you have to use that within your business process right and how AI is being used and so and what's going to happen is that I mean I think this is true with you know when someone said in terms of you know people they said that the people who leverage AI will be will will be more effective than those who don't leverage AI and that will still for organizations also. Organizations that leverage AI in fundamentally in their core business processes will be more effective than those who don't right and I think that's the thing and you won't know the difference until one day it becomes too obvious and it will be too late and I think that's the reason why everybody needs to think about what it means for your business because you will everything will be fine. Everything will be fine and one day somebody in your either either you're competitive in your space or somebody brand new coming into your space will be reimagining your business process completely and at that stage you will find that it's you know it's a it's a very big very tall you know mountain to climb and that's why I think it's important for both people and entities to think about how they will you know they they will upgrade themselves or they will modify their process to that's a very nuanced answer and everybody everybody here who's running a business should really think about it because life will be the same and then suddenly suddenly something will you know then there will be a step change. We make a few more questions but I'm sure the audience has a lot of questions for you so how are we doing on time? Okay so does okay a lot of questions so love to is there a mic that we can pass on? Thank you. My name is Karthik I work for IT service industry so you're saying that you're working on LLM sorry it's it's a fine tuned LLM on top of Lama my basic question fundamental question is we don't have a foundation model for India most of the models are basically using English or the those kind of things for example you and Andrew was talking about the tokenizers and things like that so are you working on anything like that or you you want to use mostly the existing models and run on top of it? Are you going to ask a good question you ask a cherry question for himself. No I think the interesting thing is that if you look at and then we have actually a blog on this on our website I think one of the things that we've been actually built a customized tokenizer which actually fundamentally changes the cost of some of these generations in Indian languages and and I think that we're we're not just fine tuning we're actually we are leveraging the existing retaining but we are doing what's known as continual free training which actually but having said that you know I think that when we have to figure out where is the data to train an extremely large model from scratch and some of those things are things which will happen over time but I think that I think that yes I think that we will be doing various kinds of things but the interesting thing is that if I want to change the accessibility problem with an existing open source model how do I do that and that's the problem that we have that we think we have solved and and it's going to be the heart of this open-heart DCT. It's extremely well explained in the blog even I could understand it so. Hi I'm Prishant I work for a fit tech company my question is like unlike China we never had a consumer facing application coming out from India and in web one web two crypto and all why do you think it will be different this time in like AI because will the BPI and other things will sub the same purpose but the great five world did in China or do you think like in because AI is a strategic sector no outside country can work in NASA projects maybe for government content will go to them what at least the mode here for an Indian company. So I don't I think I think the the question is I don't know the answer to these questions right I mean I and I think that it's difficult to predict but I do believe and as I'm repeating that the combinatorial effect of being using JNI at a large scale in addition in along with the DPI work that we've done in India will have people and I think that in the end it is the intent is that people need to be able to use it and they will vote by things that are useful for them and if that doesn't happen you're right that I think that we have to figure out what is the mechanism of delivery of apps right I mean in wow where do Indians consume content that's a question. I'm so sorry but we are out of time they will be outside yeah so he would be able to answer the question we have time for one last question can I can I just take one last yeah thank you thank you I'm Manish Kuthari I'm from ISBR Business School good that I got a chance to ask you this question during lunchtime they were a few of our educationists whom we were talking about and we were there was one from school and we are from the MBA institutions we were thinking of these present generations how do we get them into what you are doing there is one thing that they have been regularly that the concentrations that they're working on but artificial intelligence and getting into this getting them into their academics and making them a part of it is very important including the trainers who train them making them future ready into what you are doing is amazing and the speed that which is growing it is calling for a lot of training that needs to be done can you from your angle through some light on how we could make them future ready how these people who are management graduates and from schools who are coming out how do we get into this part of technology that you spoke about oh so this is this is really a challenge because I think everyone will need to understand at some level what this technology does and I think that we have to rethink how we get everyone into these and that this kind of education has to be at many different levels right there are from a core set of having people who are extremely good at some and there you don't need as many but then there are basically vast numbers of people who can actually leverage these tools by the way the most important thing about and maybe that's part of what makes an LLM interesting is that how you use it your mileage varies by that and to understand how to actually leverage this in an interesting way is something that we have to widely teach many many people and because asking the you know things in the right way and having the right kind of applications will make a huge difference to how people can leverage these tools. Thank you. Thank you very much Vivek very good luck to Sarvam and good luck to India I think it's going to be a lot of regular news for this. Thanks, Palai. Thank you Mr. Raghavan and\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(\"T:/SarvamAI/Task1_ytVideo/audio.mp3\")\n",
    "print(result[\"text\"])    # Getting the full transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e778be",
   "metadata": {},
   "source": [
    "# Semnatic chunking of the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12264442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functionn to Transcribe audio to chunks\n",
    "def transcribe_audio_to_chunks(audio_path, chunk_duration=10):\n",
    "    result = model.transcribe(audio_path)\n",
    "    full_text = result[\"text\"]\n",
    "    total_duration = result[\"segments\"][-1][\"end\"] if result[\"segments\"] else 0\n",
    "    chunks = []\n",
    "\n",
    "    for chunk_id in range(0, int(total_duration), chunk_duration):\n",
    "        chunk_start = chunk_id\n",
    "        chunk_end = min(chunk_id + chunk_duration, total_duration)\n",
    "        chunk_text = \" \".join([segment[\"text\"].strip() for segment in result[\"segments\"] if\n",
    "                               segment[\"start\"] >= chunk_start and segment[\"end\"] <= chunk_end])\n",
    "        \n",
    "        # Converting text chunks into given format\n",
    "        chunks.append({\n",
    "            \"chunk_id\": chunk_id // chunk_duration,\n",
    "            \"chunk_length\": chunk_end - chunk_start,\n",
    "            \"text\": chunk_text,\n",
    "            \"start_time\": chunk_start,\n",
    "            \"end_time\": chunk_end\n",
    "        })\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2177fbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk_id': 0,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'Congratulations to you Mr. Raghavan for that. Thank you so much for joining us. Over to you. Hi everybody. How are you?',\n",
       "  'start_time': 0,\n",
       "  'end_time': 15},\n",
       " {'chunk_id': 1,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"Let's hear it. Are you guys awake? All right you better be because we have a superstar guest here.\",\n",
       "  'start_time': 15,\n",
       "  'end_time': 30},\n",
       " {'chunk_id': 2,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"So we're going to ask for about 40 million dollars from him by the end of this conversation. But let's get started. I want to introduce Vivek and Pratius, she's co-founder who's not here.\",\n",
       "  'start_time': 30,\n",
       "  'end_time': 45},\n",
       " {'chunk_id': 3,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'We wanted to start with a playing a video of what OpenHathe does. I encourage all of you to go to the website, www.severalm.ai and check it out. But let me start by introducing Vivek. Vivek is a',\n",
       "  'start_time': 45,\n",
       "  'end_time': 60},\n",
       " {'chunk_id': 4,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"journey Vivek, you've got a PhD from Carnegie Mellon. You've sat in and sold the company to Magma.\",\n",
       "  'start_time': 60,\n",
       "  'end_time': 75},\n",
       " {'chunk_id': 5,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"in India for the last 16 years. And what most people don't know is your journey at Adhar.\",\n",
       "  'start_time': 75,\n",
       "  'end_time': 90},\n",
       " {'chunk_id': 6,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'technology visionary behind Adhar which we all take for granted today. So please give it out.',\n",
       "  'start_time': 90,\n",
       "  'end_time': 105},\n",
       " {'chunk_id': 7,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"Vivek. And since then he also was at AI for Bharat which we're going to touch on where he met Pratiush's\",\n",
       "  'start_time': 105,\n",
       "  'end_time': 120},\n",
       " {'chunk_id': 8,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"Microsoft Research playing a key role and a faculty at IIT Madras and at AI for Bharat. So that's a little brief introduction about them. These guys are modest, modest engineers. So they don't\",\n",
       "  'start_time': 120,\n",
       "  'end_time': 135},\n",
       " {'chunk_id': 9,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"about the money. Funding. 41 million bucks man. That's a lot of money. Every entrepreneur here is\",\n",
       "  'start_time': 135,\n",
       "  'end_time': 150},\n",
       " {'chunk_id': 10,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"No, I think it's a trend of what's going on in India. I think that for the very first time,\",\n",
       "  'start_time': 150,\n",
       "  'end_time': 165},\n",
       " {'chunk_id': 11,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"And let's try to figure out how to build something as a foundational technology out of the country.\",\n",
       "  'start_time': 165,\n",
       "  'end_time': 180},\n",
       " {'chunk_id': 12,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"the past 15 years, I've been working in both digital public infrastructure and kind of non-profit kind of things. But when this whole thing of generative AI came about,\",\n",
       "  'start_time': 180,\n",
       "  'end_time': 195},\n",
       " {'chunk_id': 13,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'we said, okay, how can I actually make a difference in this space? And I said, maybe this is the opportunity to actually come out and really build something. And the only way that we realize that',\n",
       "  'start_time': 195,\n",
       "  'end_time': 210},\n",
       " {'chunk_id': 14,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'there and we said, we want to build something, which is a continuation. And fundamentally, the question is the reason of what we want to do at Server May I is we want to basically make',\n",
       "  'start_time': 210,\n",
       "  'end_time': 225},\n",
       " {'chunk_id': 15,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'And when we said that we want to do this, there was a resonance in the investment community.',\n",
       "  'start_time': 225,\n",
       "  'end_time': 240},\n",
       " {'chunk_id': 16,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"India. So we see that as as as as confidence and a responsibility. And I also hope it's a trend that that you know that there are many more people like like us who are backed. Because if you look\",\n",
       "  'start_time': 240,\n",
       "  'end_time': 255},\n",
       " {'chunk_id': 17,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"I think there is just there should be many, many more entrepreneurs who are back to do things in India. I'm going to come back to the many more entrepreneurs. I'm obviously going to ask you about\",\n",
       "  'start_time': 255,\n",
       "  'end_time': 270},\n",
       " {'chunk_id': 18,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"I mean, all of what you said, you know, two million dollars, you know, that's a good no-one of money\",\n",
       "  'start_time': 270,\n",
       "  'end_time': 285},\n",
       " {'chunk_id': 19,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"I can call the problem. I can have a perfect solution for the problem. I think in the last week I've got lots of calls, lots of people telling me how I can do.\",\n",
       "  'start_time': 285,\n",
       "  'end_time': 300},\n",
       " {'chunk_id': 20,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'No, but but but honestly, I think the key thing in this is is to putting together an amazing team. And we actually have an amazing team, but we believe that it is talent that will drive this kind of',\n",
       "  'start_time': 300,\n",
       "  'end_time': 315},\n",
       " {'chunk_id': 21,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'extremely expensive compute wise to actually do these kinds of things. And I think that those are the',\n",
       "  'start_time': 315,\n",
       "  'end_time': 330},\n",
       " {'chunk_id': 22,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"entrepreneur talent. Okay, you have like 2015 people. How much are you paying these guys? But okay, well, you want that. But let's talk about what you guys actually built. What is what is open\",\n",
       "  'start_time': 330,\n",
       "  'end_time': 345},\n",
       " {'chunk_id': 23,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'So I think open Hathi is so, so first of all, right? We come from I personally come from the open',\n",
       "  'start_time': 345,\n",
       "  'end_time': 360},\n",
       " {'chunk_id': 24,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'we need the ecosystem to be successful. And as a result of that, one of the first things we did was, hey, there are these open source large language models that exist, right? I mean, everybody knows',\n",
       "  'start_time': 360,\n",
       "  'end_time': 375},\n",
       " {'chunk_id': 25,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'bunch of open source, you know, large language models. And then we said, is there any way',\n",
       "  'start_time': 375,\n",
       "  'end_time': 390},\n",
       " {'chunk_id': 26,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'you know, language. And that is really the, you know, what we decide, what we said that can we do something like that? And is this a, you know, relatively frugal way of actually, you know, making',\n",
       "  'start_time': 390,\n",
       "  'end_time': 405},\n",
       " {'chunk_id': 27,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'if you look at the amount of data and knowledge, it is still English dominates these things.',\n",
       "  'start_time': 405,\n",
       "  'end_time': 420},\n",
       " {'chunk_id': 28,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"Indian context and all of those things in actually a, in an efficient way. And therefore, this was an attempt through that. And it's a open Hathi is, you know, is currently based on\",\n",
       "  'start_time': 420,\n",
       "  'end_time': 435},\n",
       " {'chunk_id': 29,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"sizes and things like that as part of this, as part of this series. And of course, you know, we will be building further models on those and doing other things to actually, and we'll also\",\n",
       "  'start_time': 435,\n",
       "  'end_time': 450},\n",
       " {'chunk_id': 30,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"people can can can use to things. And the, that's, that's the essence of what, what this open Hathi is. So what does it mean to people in the audience here who are either doing their own\",\n",
       "  'start_time': 450,\n",
       "  'end_time': 465},\n",
       " {'chunk_id': 31,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'Sarvam, not OpenAI. No, yeah, no, no, I think, I think the way you look at it is that we,',\n",
       "  'start_time': 465,\n",
       "  'end_time': 480},\n",
       " {'chunk_id': 32,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'We are also going to be building a platform, a platform for developers where you can actually use a combination of various different kinds of models, some which are from us, some which are open',\n",
       "  'start_time': 480,\n",
       "  'end_time': 495},\n",
       " {'chunk_id': 33,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"how to deploy, you know, generative AI applications at scale and understand and evaluate their performance in an efficient manner. And that's something that we are planning to do at this,\",\n",
       "  'start_time': 495,\n",
       "  'end_time': 510},\n",
       " {'chunk_id': 34,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'it will be available to developers, but of course those who want to start with the open source things and hack with that, of course, please go ahead and do that as well.',\n",
       "  'start_time': 510,\n",
       "  'end_time': 525},\n",
       " {'chunk_id': 35,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'See, at least the things that we are doing now, right? I mean, one of the things that when we',\n",
       "  'start_time': 525,\n",
       "  'end_time': 540},\n",
       " {'chunk_id': 36,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'and different people have, and our understanding of a full stack is that we need to know how to train models from scratch. We need to know how to kind of figure out how to deploy models to',\n",
       "  'start_time': 540,\n",
       "  'end_time': 555},\n",
       " {'chunk_id': 37,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'actually deploy population scale applications, right? So we were thinking about all of these things, but still the models we were talking about are, you know, fairly small models, they are fairly',\n",
       "  'start_time': 555,\n",
       "  'end_time': 570},\n",
       " {'chunk_id': 38,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'While these models like OpenAI and Google are obviously much bigger models, right? But we want to,',\n",
       "  'start_time': 570,\n",
       "  'end_time': 585},\n",
       " {'chunk_id': 39,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'things to make it to make it available to people. Now those models are, I mean, as I said, you know, I think that there is space for all of those things, and I think as even Sridhar was talking about',\n",
       "  'start_time': 585,\n",
       "  'end_time': 600},\n",
       " {'chunk_id': 40,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'specific tasks extremely well, probably even better than the larger models, and that is really one',\n",
       "  'start_time': 600,\n",
       "  'end_time': 615},\n",
       " {'chunk_id': 41,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"these most set of models to build any AGI, right? That's not our goal here. Our goal is to make\",\n",
       "  'start_time': 615,\n",
       "  'end_time': 630},\n",
       " {'chunk_id': 42,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'language and all of those kinds of things. And obviously all of this unique to India. But what is unique about India? I mean, like, what is, is anything special in our ecosystem that makes',\n",
       "  'start_time': 630,\n",
       "  'end_time': 645},\n",
       " {'chunk_id': 43,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'So I think that, I mean, there are quite a few things that are unique about India, right? The first thing is, I think, that we are a voice first nation. So therefore, I think voice',\n",
       "  'start_time': 645,\n",
       "  'end_time': 660},\n",
       " {'chunk_id': 44,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'conscious country from from from from a cost perspective. Now there, I would say that there are',\n",
       "  'start_time': 660,\n",
       "  'end_time': 675},\n",
       " {'chunk_id': 45,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"when we're depending on your application. But when you want to scale things to a massive level and make it work, then then you have to figure out how small models work. So that's something that\",\n",
       "  'start_time': 675,\n",
       "  'end_time': 690},\n",
       " {'chunk_id': 46,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'India has had in building all this digital public infrastructure. When you add the AI layer on top of',\n",
       "  'start_time': 690,\n",
       "  'end_time': 705},\n",
       " {'chunk_id': 47,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"based on doing things like that. That's a phenomenal point. Like, you know, it's like dpi to the power of AI almost in some ways. And as a part of other building other, no better person than you.\",\n",
       "  'start_time': 705,\n",
       "  'end_time': 720},\n",
       " {'chunk_id': 48,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"Indic specific language data suited for Indian problems at a compelling cost point will be suited for us. We're not solving some world autonomous vehicles or some complex problem. We're solving\",\n",
       "  'start_time': 720,\n",
       "  'end_time': 735},\n",
       " {'chunk_id': 49,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'see as the future. Am I paraphrasing this correctly? No, yeah. So I think that certainly, I mean, voice and Indian languages are an important part of our strategy, but we will be building',\n",
       "  'start_time': 735,\n",
       "  'end_time': 750},\n",
       " {'chunk_id': 50,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'I think, in different domains, working in different domains, making building things based on',\n",
       "  'start_time': 750,\n",
       "  'end_time': 765},\n",
       " {'chunk_id': 51,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"Fair enough. So coming back to the elephant in the room, no fun intended with OpenHathi, what about Bavishakarwal and Krutrim? What does your take on that? I think it's great. I think it's\",\n",
       "  'start_time': 765,\n",
       "  'end_time': 780},\n",
       " {'chunk_id': 52,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'need multiple people working on it. The fact that there are other people thinking is actually',\n",
       "  'start_time': 780,\n",
       "  'end_time': 795},\n",
       " {'chunk_id': 53,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"together and do that. So I really welcome that. I think it's great. And I think that there'll be different people who will have different takes as to how to solve this kind of problem. And hopefully,\",\n",
       "  'start_time': 795,\n",
       "  'end_time': 810},\n",
       " {'chunk_id': 54,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"some of the predictions that you've boldly made. So Vivek, I usually ask people about what do you think the future will be and everybody usually hedges? I ask Vivek, what do you think is going to happen\",\n",
       "  'start_time': 810,\n",
       "  'end_time': 825},\n",
       " {'chunk_id': 55,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'made three bold predictions. So I want to talk about that before that I have one last question. What are the top three applications that you think are relevant for India? You would see the',\n",
       "  'start_time': 825,\n",
       "  'end_time': 840},\n",
       " {'chunk_id': 56,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'So I think that as you said, things like education and medical are clearly areas where I think',\n",
       "  'start_time': 840,\n",
       "  'end_time': 855},\n",
       " {'chunk_id': 57,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"major application where things can happen. And here I'm talking about country-specific. And I\",\n",
       "  'start_time': 855,\n",
       "  'end_time': 870},\n",
       " {'chunk_id': 58,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"and clearly we have a very large software industry and how to reimagine those things in this context is also something that's going to be big. Fair enough. Are you guys ready for Vivek Ragavan's bold\",\n",
       "  'start_time': 870,\n",
       "  'end_time': 885},\n",
       " {'chunk_id': 59,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"guys that I know. He wants to make three predictions. You don't want to hear it. All right. So I asked him, what do you think, you know, year later, what do you think we can expect?\",\n",
       "  'start_time': 885,\n",
       "  'end_time': 900},\n",
       " {'chunk_id': 60,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"questions like this because they don't want to be caught wrong. Not Vivek. Vivek is bold. So he basically said three things and I'm going to list out the three things and then he's\",\n",
       "  'start_time': 900,\n",
       "  'end_time': 915},\n",
       " {'chunk_id': 61,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"than a real person because they'll give me a better answer. So that is Vivek Ragavan's prediction.\",\n",
       "  'start_time': 915,\n",
       "  'end_time': 930},\n",
       " {'chunk_id': 62,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"that there'll be a GPU glutton India. He thinks there'll be too much GPU. So if you want a short and media stock, this is a good time. And number three, which was extremely unexpected,\",\n",
       "  'start_time': 930,\n",
       "  'end_time': 945},\n",
       " {'chunk_id': 63,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"So you want to quickly talk about each of them. Why you just came up with these and then we'll throw the audience questions. So I don't think I quite said it the way that that\",\n",
       "  'start_time': 945,\n",
       "  'end_time': 960},\n",
       " {'chunk_id': 64,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"that we said is I think that and I don't think that this is I think there will come a time when\",\n",
       "  'start_time': 960,\n",
       "  'end_time': 975},\n",
       " {'chunk_id': 65,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'today when you call when you call some kind of a bot, you actually end up, you mostly try to',\n",
       "  'start_time': 975,\n",
       "  'end_time': 990},\n",
       " {'chunk_id': 66,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"there will come a time and I'm predicting it sooner than later that you will actually get better responses from the bot and what the human representative that at least the average human\",\n",
       "  'start_time': 990,\n",
       "  'end_time': 1005},\n",
       " {'chunk_id': 67,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"that there will come a time where you know it's not a human you're talking to. But it's probably\",\n",
       "  'start_time': 1005,\n",
       "  'end_time': 1020},\n",
       " {'chunk_id': 68,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"could happen. Definitely controversial. But we'll let it go. What about the GPU glut?\",\n",
       "  'start_time': 1020,\n",
       "  'end_time': 1035},\n",
       " {'chunk_id': 69,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'I think that shortage will ease because that is how the cycles of things go, right? When I think the fact that there was such a severe shortage last year, you know basically caused',\n",
       "  'start_time': 1035,\n",
       "  'end_time': 1050},\n",
       " {'chunk_id': 70,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'always go in a cycle. But you may we may find out that there are many many more interesting problems',\n",
       "  'start_time': 1050,\n",
       "  'end_time': 1065},\n",
       " {'chunk_id': 71,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'genie event in Bangalore and we were talking to people and we said you know how many people have access to you know four a hundred. This was the question that I asked and nobody in the room',\n",
       "  'start_time': 1065,\n",
       "  'end_time': 1080},\n",
       " {'chunk_id': 72,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'thing is going to change. You will be able to get these kinds of things and people who want to hack and do things will have access to these things at in without you know having to write a',\n",
       "  'start_time': 1080,\n",
       "  'end_time': 1095},\n",
       " {'chunk_id': 73,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"the other. So I would take his predictions very seriously. So I don't know what I I'm going to sell my immediate stock. I would not do that. But that's not what I said. I want to blame you for this.\",\n",
       "  'start_time': 1095,\n",
       "  'end_time': 1110},\n",
       " {'chunk_id': 74,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'But the third one is pretty strange. You know companies are born companies die but you said some companies will suddenly die. What does that mean? No I think see I think the the interesting thing is',\n",
       "  'start_time': 1110,\n",
       "  'end_time': 1125},\n",
       " {'chunk_id': 75,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'to use that and you have to use that within your business process right and how AI is being used',\n",
       "  'start_time': 1125,\n",
       "  'end_time': 1140},\n",
       " {'chunk_id': 76,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'said in terms of you know people they said that the people who leverage AI will be will will be more',\n",
       "  'start_time': 1140,\n",
       "  'end_time': 1155},\n",
       " {'chunk_id': 77,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"that leverage AI in fundamentally in their core business processes will be more effective than those who don't right and I think that's the thing and you won't know the difference until one day\",\n",
       "  'start_time': 1155,\n",
       "  'end_time': 1170},\n",
       " {'chunk_id': 78,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'think about what it means for your business because you will everything will be fine. Everything',\n",
       "  'start_time': 1170,\n",
       "  'end_time': 1185},\n",
       " {'chunk_id': 79,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"brand new coming into your space will be reimagining your business process completely and at that stage you will find that it's you know it's a it's a very big very tall you know mountain to climb\",\n",
       "  'start_time': 1185,\n",
       "  'end_time': 1200},\n",
       " {'chunk_id': 80,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"you know they they will upgrade themselves or they will modify their process to that's a very nuanced answer and everybody everybody here who's running a business should really\",\n",
       "  'start_time': 1200,\n",
       "  'end_time': 1215},\n",
       " {'chunk_id': 81,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"then there will be a step change. We make a few more questions but I'm sure the audience has a\",\n",
       "  'start_time': 1215,\n",
       "  'end_time': 1230},\n",
       " {'chunk_id': 82,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'love to is there a mic that we can pass on? Thank you. My name is Karthik I work for',\n",
       "  'start_time': 1230,\n",
       "  'end_time': 1245},\n",
       " {'chunk_id': 83,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"LLM sorry it's it's a fine tuned LLM on top of Lama my basic question fundamental question is\",\n",
       "  'start_time': 1245,\n",
       "  'end_time': 1260},\n",
       " {'chunk_id': 84,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'the those kind of things for example you and Andrew was talking about the tokenizers and things',\n",
       "  'start_time': 1260,\n",
       "  'end_time': 1275},\n",
       " {'chunk_id': 85,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'and run on top of it? Are you going to ask a good question you ask a cherry question for himself. No I think the interesting thing is that if you look at and then we have actually a blog on this',\n",
       "  'start_time': 1275,\n",
       "  'end_time': 1290},\n",
       " {'chunk_id': 86,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"which actually fundamentally changes the cost of some of these generations in Indian languages and and I think that we're we're not just fine tuning we're actually we are leveraging the\",\n",
       "  'start_time': 1290,\n",
       "  'end_time': 1305},\n",
       " {'chunk_id': 87,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'having said that you know I think that when we have to figure out where is the data to train an',\n",
       "  'start_time': 1305,\n",
       "  'end_time': 1320},\n",
       " {'chunk_id': 88,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'but I think that I think that yes I think that we will be doing various kinds of things but the interesting thing is that if I want to change the accessibility problem with an existing',\n",
       "  'start_time': 1320,\n",
       "  'end_time': 1335},\n",
       " {'chunk_id': 89,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"and and it's going to be the heart of this open-heart DCT. It's extremely well explained in the blog even I could understand it so.\",\n",
       "  'start_time': 1335,\n",
       "  'end_time': 1350},\n",
       " {'chunk_id': 90,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'consumer facing application coming out from India and in web one web two crypto and all',\n",
       "  'start_time': 1350,\n",
       "  'end_time': 1365},\n",
       " {'chunk_id': 91,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'sub the same purpose but the great five world did in China or do you think like in because AI is a',\n",
       "  'start_time': 1365,\n",
       "  'end_time': 1380},\n",
       " {'chunk_id': 92,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'go to them what at least the mode here for an Indian company.',\n",
       "  'start_time': 1380,\n",
       "  'end_time': 1395},\n",
       " {'chunk_id': 93,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"mean I and I think that it's difficult to predict but I do believe and as I'm repeating that\",\n",
       "  'start_time': 1395,\n",
       "  'end_time': 1410},\n",
       " {'chunk_id': 94,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"DPI work that we've done in India will have people and I think that in the end it is the intent is that people need to be able to use it and they will vote by things that are useful for them and\",\n",
       "  'start_time': 1410,\n",
       "  'end_time': 1425},\n",
       " {'chunk_id': 95,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"of delivery of apps right I mean in wow where do Indians consume content that's a question.\",\n",
       "  'start_time': 1425,\n",
       "  'end_time': 1440},\n",
       " {'chunk_id': 96,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"question we have time for one last question can I can I just take one last yeah thank you thank you I'm Manish Kuthari I'm from ISBR Business School good that I got a chance to ask you this question\",\n",
       "  'start_time': 1440,\n",
       "  'end_time': 1455},\n",
       " {'chunk_id': 97,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'was one from school and we are from the MBA institutions we were thinking of these present generations how do we get them into what you are doing there is one thing that they have been',\n",
       "  'start_time': 1455,\n",
       "  'end_time': 1470},\n",
       " {'chunk_id': 98,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'into this getting them into their academics and making them a part of it is very important including the trainers who train them making them future ready into what you are doing is amazing and the',\n",
       "  'start_time': 1470,\n",
       "  'end_time': 1485},\n",
       " {'chunk_id': 99,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'from your angle through some light on how we could make them future ready how these people who are',\n",
       "  'start_time': 1485,\n",
       "  'end_time': 1500},\n",
       " {'chunk_id': 100,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'that you spoke about oh so this is this is really a challenge because I think everyone will need to',\n",
       "  'start_time': 1500,\n",
       "  'end_time': 1515},\n",
       " {'chunk_id': 101,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"everyone into these and that this kind of education has to be at many different levels right there are from a core set of having people who are extremely good at some and there you don't need\",\n",
       "  'start_time': 1515,\n",
       "  'end_time': 1530},\n",
       " {'chunk_id': 102,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"by the way the most important thing about and maybe that's part of what makes an LLM interesting\",\n",
       "  'start_time': 1530,\n",
       "  'end_time': 1545},\n",
       " {'chunk_id': 103,\n",
       "  'chunk_length': 15,\n",
       "  'text': 'in an interesting way is something that we have to widely teach many many people and because asking the you know things in the right way and having the right kind of applications will make',\n",
       "  'start_time': 1545,\n",
       "  'end_time': 1560},\n",
       " {'chunk_id': 104,\n",
       "  'chunk_length': 15,\n",
       "  'text': \"good luck to Sarvam and good luck to India I think it's going to be a lot of regular news for this. Thanks, Palai.\",\n",
       "  'start_time': 1560,\n",
       "  'end_time': 1575}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcribe_audio_to_chunks(\"T:/SarvamAI/Task1_ytVideo/audio.mp3\", 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc1865c",
   "metadata": {},
   "source": [
    "Creating a basic Gradio Interface for the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af4ad868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\tanay\\anaconda3\\lib\\site-packages (4.31.5)\n",
      "Requirement already satisfied: ruff>=0.2.2 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (0.4.4)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (6.4.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (0.29.0)\n",
      "Requirement already satisfied: gradio-client==0.16.4 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (0.16.4)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (3.9.0)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (2.7.1)\n",
      "Requirement already satisfied: pydub in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (4.11.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (3.10.3)\n",
      "Requirement already satisfied: urllib3~=2.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (2.2.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (0.23.1)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (5.1.1)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (9.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (23.1)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (0.3.2)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: numpy~=1.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (1.23.5)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (1.5.3)\n",
      "Requirement already satisfied: fastapi in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio) (0.111.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio-client==0.16.4->gradio) (11.0.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from gradio-client==0.16.4->gradio) (2024.5.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (3.5.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
      "Requirement already satisfied: idna in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (3.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: certifi in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (2022.12.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2022.7)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.5.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from fastapi->gradio) (5.4.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from fastapi->gradio) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from fastapi->gradio) (0.0.4)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from fastapi->gradio) (2.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.4)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from uvicorn>=0.14.0->gradio) (1.0.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from uvicorn>=0.14.0->gradio) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\tanay\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\tanay\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02943b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from pytube import YouTube \n",
    "from moviepy.editor import AudioFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6adb2d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this block of code all the functions which we are going to use in Gradio interface are defined\n",
    "\n",
    "# Download audio from YouTube in order to perform transcription on it\n",
    "def download_audio_from_youtube(youtube_url):\n",
    "    yt = YouTube(youtube_url) # getting the yt url\n",
    "    audio_stream = yt.streams.filter(only_audio=True).first() # seperation of audio stream \n",
    "    audio_file_path = audio_stream.download(filename=\"youtube_audio.mp4\")\n",
    "    audio_clip = AudioFileClip(audio_file_path)\n",
    "    audio_clip.write_audiofile(\"youtube_audio.wav\", codec='pcm_s16le')\n",
    "    return \"youtube_audio.wav\"\n",
    "\n",
    "\n",
    "# Download video from YouTube (to diaplay the actual video on gradio interface)\n",
    "def download_video_from_youtube(youtube_url):\n",
    "    yt = YouTube(youtube_url)\n",
    "    video_stream = yt.streams.filter(progressive=True, file_extension=\"mp4\").first()\n",
    "    video_file_path = video_stream.download(filename=\"youtube_video.mp4\")\n",
    "    return video_file_path\n",
    "\n",
    "# Gradio interface\n",
    "def process_youtube_link(youtube_url):\n",
    "    # defining the audio and video file paths\n",
    "    audio_path = download_audio_from_youtube(youtube_url)\n",
    "    video_path = download_video_from_youtube(youtube_url)\n",
    "    # getting chunks of the audio with the \"transcribe_audio_to_chunks(audio_path)\" function which is defined above\n",
    "    chunks = transcribe_audio_to_chunks(audio_path)\n",
    "\n",
    "    # Preparing chunks for display\n",
    "    formatted_chunks = \"\".join([\n",
    "        f\"<span id='chunk-{chunk['chunk_id']}'>{chunk['text']} (Start: {chunk['start_time']}s, End: {chunk['end_time']}s)</span><br><br>\"\n",
    "        for chunk in chunks\n",
    "    ])\n",
    "\n",
    "    return video_path, formatted_chunks, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caea4d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JavaScript code for syncing text highlighting with video playback\n",
    "js_code = \"\"\"\n",
    "const videoPlayer = document.querySelector(\"video\");\n",
    "const transcriptionOutput = document.getElementById(\"transcription-output\");\n",
    "\n",
    "videoPlayer.addEventListener(\"timeupdate\", () => {\n",
    "    const currentTime = videoPlayer.currentTime;\n",
    "    const chunks = JSON.parse(document.getElementById(\"chunks-data\").innerText);\n",
    "\n",
    "    chunks.forEach(chunk => {\n",
    "        const chunkElement = document.getElementById(`chunk-${chunk.chunk_id}`);\n",
    "        if (currentTime >= chunk.start_time && currentTime <= chunk.end_time) {\n",
    "            chunkElement.style.backgroundColor = \"yellow\";\n",
    "        } else {\n",
    "            chunkElement.style.backgroundColor = \"transparent\";\n",
    "        }\n",
    "    });\n",
    "});\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# creating and launching Gradio interface. Initializing buttons and their functions.\n",
    "with gr.Blocks() as demo:\n",
    "    youtube_link = gr.Textbox(label=\"Enter YouTube URL\")\n",
    "    submit_button = gr.Button(\"Submit\")\n",
    "    video_output = gr.Video(label=\"Downloaded Video\")\n",
    "    transcription_output = gr.HTML(label=\"Transcribed Chunks\")\n",
    "    hidden_chunks_data = gr.HTML(visible=False)\n",
    "\n",
    "    submit_button.click(fn=process_youtube_link, inputs=youtube_link,\n",
    "                        outputs=[video_output, transcription_output, hidden_chunks_data])\n",
    "\n",
    "    # Add JavaScript to the interface\n",
    "    gr.HTML(value=f\"\"\"\n",
    "    <script>{js_code}</script>\n",
    "    \"\"\")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2b566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
